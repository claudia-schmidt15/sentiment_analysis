{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from typing import List, Dict, Tuple\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG-Q-ax0W9Ft",
        "outputId": "354d10ae-fc55-441c-9c6b-2835ba733d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk import pos_tag\n",
        "from pathlib import Path\n",
        "\n",
        "def setup_nltk():\n",
        "    print(f\"Using NLTK version: {nltk.__version__}\")\n",
        "\n",
        "    # Step 1: Set up directory\n",
        "    nltk_data_dir = Path('/root/nltk_data')\n",
        "\n",
        "    # Step 2: Set environment variable\n",
        "    os.environ['NLTK_DATA'] = str(nltk_data_dir)\n",
        "\n",
        "    # Step 3: Download correct packages\n",
        "    print(\"\\nDownloading NLTK packages...\")\n",
        "    packages = [\n",
        "        'punkt',\n",
        "        'averaged_perceptron_tagger_eng',  # Use the correct package name\n",
        "        'universal_tagset'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        nltk.download(package, download_dir=str(nltk_data_dir))\n",
        "        print(f\"✓ Downloaded {package}\")\n",
        "\n",
        "    # Step 4: Verify installation\n",
        "    print(\"\\nVerifying installation...\")\n",
        "    test_sentence = \"Students struggle with exams and worry about grades.\"\n",
        "\n",
        "    # Test tokenization\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    tokens = tokenizer.tokenize(test_sentence)\n",
        "    print(\"✓ Tokenization successful:\", tokens)\n",
        "\n",
        "    # Test POS tagging\n",
        "    tags = pos_tag(tokens)\n",
        "    print(\"\\n✓ POS tagging successful:\")\n",
        "    for word, tag in tags:\n",
        "        print(f\"{word}: {tag}\")\n",
        "\n",
        "    # Show nouns and verbs specifically\n",
        "    nouns_verbs = [(word, tag) for word, tag in tags\n",
        "                   if tag.startswith(('NN', 'VB'))]\n",
        "\n",
        "    print(\"\\nIdentified nouns and verbs:\")\n",
        "    for word, tag in nouns_verbs:\n",
        "        print(f\"{word}: {tag}\")\n",
        "\n",
        "    print(\"\\n✓ Setup completed successfully!\")\n",
        "    return True\n",
        "\n",
        "# Run the setup\n",
        "if __name__ == \"__main__\":\n",
        "    setup_nltk()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ih-RAog-8U",
        "outputId": "0e315223-1a24-4959-fe14-40aa4d776963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using NLTK version: 3.9.1\n",
            "\n",
            "Downloading NLTK packages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Downloaded punkt\n",
            "✓ Downloaded averaged_perceptron_tagger_eng\n",
            "✓ Downloaded universal_tagset\n",
            "\n",
            "Verifying installation...\n",
            "✓ Tokenization successful: ['Students', 'struggle', 'with', 'exams', 'and', 'worry', 'about', 'grades', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ POS tagging successful:\n",
            "Students: NNS\n",
            "struggle: VBP\n",
            "with: IN\n",
            "exams: NNS\n",
            "and: CC\n",
            "worry: VBP\n",
            "about: IN\n",
            "grades: NNS\n",
            ".: .\n",
            "\n",
            "Identified nouns and verbs:\n",
            "Students: NNS\n",
            "struggle: VBP\n",
            "exams: NNS\n",
            "worry: VBP\n",
            "grades: NNS\n",
            "\n",
            "✓ Setup completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration parameters for the analysis\"\"\"\n",
        "    model_name: str = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "    min_posts: int = 1000  # Minimum posts per college\n",
        "    year: str = \"2019\"\n",
        "    max_sequence_length: int = 512\n",
        "    num_clusters: int = 5\n",
        "    batch_size: int = 32\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Paths\n",
        "    base_path: Path = Path('/content/drive/My Drive/CS470_GroupProject')\n",
        "    data_path: Path = base_path / 'roberta2022'\n",
        "    output_path: Path = base_path / 'results'\n",
        "\n",
        "# Initialize config\n",
        "config = Config()\n",
        "config.output_path.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "62LCcknfXCqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "3fcdaeb4-09c3-4a09-bb41-c4d8e6a14333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/CS470_GroupProject/results'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-decfe223e22a>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Initialize config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \"\"\"\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/CS470_GroupProject/results'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"Handles data loading and preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_and_filter_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load and filter data for specific year and minimum posts\"\"\"\n",
        "        dfs = []\n",
        "\n",
        "        for file in self.config.data_path.glob(\"*.csv\"):\n",
        "            if self.config.year not in file.name:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(file)\n",
        "                if len(df) >= self.config.min_posts:\n",
        "                    df['college'] = file.stem.split('_')[0]  # Extract college name\n",
        "                    dfs.append(df)\n",
        "                    logger.info(f\"Loaded {len(df)} posts from {file.name}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading {file}: {str(e)}\")\n",
        "\n",
        "        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
      ],
      "metadata": {
        "id": "7hZvFF6XXG4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    \"\"\"Handles sentiment analysis and attention weight extraction\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            config.model_name\n",
        "        ).to(self.device)\n",
        "        self.model.eval()\n",
        "        logger.info(\"Model loaded and moved to GPU\")\n",
        "\n",
        "    def process_batch(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Process a batch of texts and return sentiment scores and attention weights\"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(\n",
        "                texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=self.config.max_sequence_length\n",
        "            ).to(self.device)\n",
        "\n",
        "            outputs = self.model(**inputs, output_attentions=True)\n",
        "\n",
        "            # Get attention weights from last layer\n",
        "            attentions = outputs.attentions[-1].mean(dim=1).cpu().numpy()\n",
        "\n",
        "            return attentions\n",
        "\n",
        "    def analyze_posts(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Analyze posts and extract attention patterns\"\"\"\n",
        "        # Find all negative posts first\n",
        "        negative_posts = df[\n",
        "            (df['emo_pred_neg'] > df['emo_pred_pos']) &\n",
        "            (df['emo_pred_neg'] > df['emo_pred_neu'])\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nTotal posts in dataset: {len(df)}\")\n",
        "        print(f\"Total negative posts: {len(negative_posts)}\")\n",
        "        print(f\"Percentage negative: {(len(negative_posts)/len(df))*100:.2f}%\\n\")\n",
        "\n",
        "        results = defaultdict(list)\n",
        "        processed_count = 0\n",
        "\n",
        "        # Process only the negative posts\n",
        "        for i in range(0, len(negative_posts), self.config.batch_size):\n",
        "            batch_texts = negative_posts['body'].iloc[i:i + self.config.batch_size].tolist()\n",
        "            attentions = self.process_batch(batch_texts)\n",
        "\n",
        "            for j, (text, attention) in enumerate(zip(batch_texts, attentions)):\n",
        "                tokens = self.tokenizer.convert_ids_to_tokens(\n",
        "                    self.tokenizer(text, truncation=True)['input_ids']\n",
        "                )\n",
        "\n",
        "                # Get attention scores for each token\n",
        "                token_attention = list(zip(tokens, attention.mean(axis=0).tolist()))\n",
        "\n",
        "                results['texts'].append(text)\n",
        "                results['colleges'].append(negative_posts.iloc[i + j]['college'])\n",
        "                results['attention_patterns'].append(token_attention)\n",
        "\n",
        "            processed_count += len(batch_texts)\n",
        "            print(f\"Processed {processed_count}/{len(negative_posts)} negative posts\")\n",
        "\n",
        "        logger.info(f\"Attention analysis complete for {processed_count} negative posts\")\n",
        "        return dict(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X366NOiwXJ_x",
        "outputId": "550e8048-b12d-49d5-d5be-d81fa5cc7326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import pandas as pd  # Ensure pandas is imported\n",
        "import re  # For token cleanup\n",
        "\n",
        "class ThemeAnalyzer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.n_clusters = 4\n",
        "\n",
        "    def create_matrix_from_raw_data(self, attention_data, pos_tags):\n",
        "        \"\"\"Create matrix and dataframe from raw data\"\"\"\n",
        "        print(\"Creating matrix from attention patterns and POS tags...\\n\")\n",
        "\n",
        "        # Preview the JSON data for understanding\n",
        "        print(\"Preview of 'attention_data':\")\n",
        "        print({key: attention_data[key][:2] if isinstance(attention_data[key], list) else attention_data[key]\n",
        "               for key in list(attention_data.keys())[:3]})  # Display first 2 entries of first 3 keys\n",
        "\n",
        "        print(\"\\nPreview of 'pos_tags':\")\n",
        "        print({key: pos_tags[key][:2] for key in list(pos_tags.keys())[:3]})  # Display first 2 entries for first 3 documents\n",
        "\n",
        "        # Process documents\n",
        "        processed_docs = []\n",
        "        vocab = set()\n",
        "        word_to_idx = {}\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        top_words_examples = []  # To store examples of top words for printing later\n",
        "\n",
        "        def clean_token(token):\n",
        "            \"\"\"Clean token by removing unwanted characters and artifacts.\"\"\"\n",
        "            token = re.sub(r\"[^\\w]\", \"\", token)  # Remove non-alphanumeric characters\n",
        "            return token.lower().strip()  # Convert to lowercase and strip whitespace\n",
        "\n",
        "        for idx, (text, college, patterns) in enumerate(zip(\n",
        "            attention_data['texts'],\n",
        "            attention_data['colleges'],\n",
        "            attention_data['attention_patterns']\n",
        "        )):\n",
        "            doc_id = f\"{college}_{idx}\"\n",
        "            if doc_id not in pos_tags:\n",
        "                continue\n",
        "\n",
        "            # Get POS tags for this document\n",
        "            doc_pos_tags = pos_tags[doc_id]\n",
        "\n",
        "            # Filter for nouns and verbs with weights\n",
        "            noun_verb_weights = []\n",
        "            for (token, weight), (_, pos) in zip(patterns, doc_pos_tags):\n",
        "                cleaned_token = clean_token(token)\n",
        "                if pos.startswith(('NN', 'VB')) and cleaned_token and cleaned_token != \"s\":\n",
        "                    noun_verb_weights.append((cleaned_token, float(weight)))\n",
        "\n",
        "            if noun_verb_weights:\n",
        "                # Sort by weight and keep top 5% (after filtering out <s>)\n",
        "                noun_verb_weights.sort(key=lambda x: x[1], reverse=True)\n",
        "                cutoff = max(1, int(len(noun_verb_weights) * 0.05))\n",
        "                top_words = noun_verb_weights[:cutoff]\n",
        "\n",
        "                # Collect examples for later printing\n",
        "                if len(top_words_examples) < 5:  # Store examples for 5 documents\n",
        "                    top_words_examples.append((idx, top_words))\n",
        "\n",
        "                # Update vocabulary\n",
        "                for word, _ in top_words:\n",
        "                    if word not in word_to_idx:\n",
        "                        word_to_idx[word] = len(word_to_idx)\n",
        "\n",
        "                # Add to matrix construction lists\n",
        "                for word, weight in top_words:\n",
        "                    rows.append(len(processed_docs))\n",
        "                    cols.append(word_to_idx[word])\n",
        "                    data.append(weight)\n",
        "\n",
        "                processed_docs.append({\n",
        "                    'college': college,\n",
        "                    'text': text,\n",
        "                    'important_words': top_words\n",
        "                })\n",
        "\n",
        "            if idx % 1000 == 0:\n",
        "                print(f\"Processed {idx}/{len(attention_data['texts'])} documents\")\n",
        "\n",
        "        # Create sparse matrix\n",
        "        X = csr_matrix((data, (rows, cols)),\n",
        "                       shape=(len(processed_docs), len(word_to_idx)))\n",
        "\n",
        "        # Create dataframe\n",
        "        df = pd.DataFrame(processed_docs)\n",
        "\n",
        "        # Print top words examples\n",
        "        print(\"\\nExamples of top-ranked nouns and verbs before clustering:\")\n",
        "        for doc_idx, words in top_words_examples:\n",
        "            print(f\"Document {doc_idx}:\")\n",
        "            print(\", \".join([f\"{word} ({weight:.2f})\" for word, weight in words]))\n",
        "\n",
        "        print(f\"\\nCreated matrix of shape {X.shape}\")\n",
        "        return X, df\n",
        "\n",
        "    def process_existing_matrix(self, X, df):\n",
        "        \"\"\"Process existing sparse matrix with fixed clusters\"\"\"\n",
        "        print(f\"\\nProcessing matrix of shape {X.shape} with {self.n_clusters} clusters\")\n",
        "\n",
        "        # Check for CUDA\n",
        "        try:\n",
        "            import cupy as cp\n",
        "            from cuml.cluster import KMeans as cuKMeans\n",
        "            print(\"CUDA available - using GPU acceleration\")\n",
        "\n",
        "            # Convert to GPU\n",
        "            X_gpu = cp.sparse.csr_matrix(X)\n",
        "            kmeans = cuKMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto')\n",
        "            df['cluster'] = kmeans.fit_predict(X_gpu)\n",
        "            cluster_centers = cp.asnumpy(kmeans.cluster_centers_)\n",
        "\n",
        "        except (ImportError, ModuleNotFoundError):\n",
        "            print(\"CUDA not available - using CPU\")\n",
        "            from sklearn.cluster import KMeans\n",
        "\n",
        "            kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto')\n",
        "            df['cluster'] = kmeans.fit_predict(X)\n",
        "            cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "        # Analyze clusters\n",
        "        print(\"\\nAnalyzing clusters...\")\n",
        "        themes = {}\n",
        "\n",
        "        for i in range(self.n_clusters):\n",
        "            cluster_docs = df[df['cluster'] == i]\n",
        "\n",
        "            # Aggregate word weights in cluster\n",
        "            word_weights = defaultdict(float)\n",
        "            for words in cluster_docs['important_words']:\n",
        "                for word, weight in words:\n",
        "                    word_weights[word] += weight\n",
        "\n",
        "            # Get top words by total weight\n",
        "            top_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "\n",
        "            themes[i] = {\n",
        "                'size': len(cluster_docs),\n",
        "                'percentage': len(cluster_docs) / len(df) * 100,\n",
        "                'top_words': top_words,\n",
        "                'sample_posts': cluster_docs['text'].sample(min(5, len(cluster_docs))).tolist(),\n",
        "                'colleges': cluster_docs['college'].value_counts().to_dict()\n",
        "            }\n",
        "\n",
        "            print(f\"\\nCluster {i}:\")\n",
        "            print(f\"Size: {len(cluster_docs)} posts ({themes[i]['percentage']:.1f}%)\")\n",
        "            print(\"Top words (weight):\", ', '.join([f\"{w}({s:.2f})\" for w, s in top_words[:5]]))\n",
        "            print(\"Sample post:\", themes[i]['sample_posts'][0][:200] + \"...\")\n",
        "\n",
        "        return {\n",
        "            'cluster_themes': themes,\n",
        "            'metadata': {\n",
        "                'total_posts': len(df),\n",
        "                'n_clusters': self.n_clusters,\n",
        "                'vocabulary_size': X.shape[1]\n",
        "            }\n",
        "        }\n"
      ],
      "metadata": {
        "id": "gV2T6712PSGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function starting from JSON files\"\"\"\n",
        "    try:\n",
        "        # Initialize config\n",
        "        config = Config()\n",
        "\n",
        "        # Load our saved files\n",
        "        logger.info(\"Loading saved data...\")\n",
        "        with open(config.output_path / 'attention_patterns.json', 'r') as f:\n",
        "            attention_data = json.load(f)\n",
        "        with open(config.output_path / 'pos_tags.json', 'r') as f:\n",
        "            pos_tags = json.load(f)\n",
        "\n",
        "        logger.info(f\"Loaded data for {len(attention_data['texts'])} documents\")\n",
        "\n",
        "        # Create matrix and run clustering\n",
        "        theme_analyzer = ThemeAnalyzer(config)\n",
        "\n",
        "        # First create matrix\n",
        "        X, df = theme_analyzer.create_matrix_from_raw_data(attention_data, pos_tags)\n",
        "\n",
        "        # Then do clustering\n",
        "        logger.info(\"Starting clustering with 4 clusters...\")\n",
        "        cluster_results = theme_analyzer.process_existing_matrix(X, df)\n",
        "\n",
        "        # Save results\n",
        "        results_file = config.output_path / 'theme_clusters.json'\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(cluster_results, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Analysis complete! Results saved to {results_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import json\n",
        "    import logging\n",
        "    import pandas as pd\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQr-cSyaXPhJ",
        "outputId": "06669239-05a3-4819-d235-3260d759bdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating matrix from attention patterns and POS tags...\n",
            "\n",
            "Preview of 'attention_data':\n",
            "{'texts': ['I wish. Most of the classes they didn’t accept were either dual credit classes that I took in HS or courses I took P/F. Additionally I had an internship that was worth 4cr that didn’t transfer in, and a required freshman seminar from my last college didn’t translate to anything.\\n\\nIt’s fair. But it sucks.', 'The university no longer does guaranteed longevity pay due to annual increases of the minimum wage. :('], 'colleges': ['BostonU', 'BostonU'], 'attention_patterns': [[['<s>', 0.2755829691886902], ['I', 0.008264525793492794], ['Ġwish', 0.00782431848347187], ['.', 0.007175407372415066], ['ĠMost', 0.008431607857346535], ['Ġof', 0.005328712053596973], ['Ġthe', 0.006881313864141703], ['Ġclasses', 0.00990503840148449], ['Ġthey', 0.009608980268239975], ['Ġdidn', 0.005576973780989647], ['âĢ', 0.005375291220843792], ['Ļ', 0.006462315563112497], ['t', 0.004150718916207552], ['Ġaccept', 0.006718026008456945], ['Ġwere', 0.011224303394556046], ['Ġeither', 0.007425454445183277], ['Ġdual', 0.008732195943593979], ['Ġcredit', 0.006546749267727137], ['Ġclasses', 0.0074068233370780945], ['Ġthat', 0.0033725134562700987], ['ĠI', 0.009882007725536823], ['Ġtook', 0.008745466358959675], ['Ġin', 0.0038660052232444286], ['ĠHS', 0.009410522878170013], ['Ġor', 0.004941639490425587], ['Ġcourses', 0.00880610290914774], ['ĠI', 0.006218569818884134], ['Ġtook', 0.0075695207342505455], ['ĠP', 0.0057653444819152355], ['/', 0.0035808782558888197], ['F', 0.005774963181465864], ['.', 0.00841479655355215], ['ĠAdditionally', 0.006530601531267166], ['ĠI', 0.010978195816278458], ['Ġhad', 0.005679119378328323], ['Ġan', 0.006720391102135181], ['Ġinternship', 0.005499863065779209], ['Ġthat', 0.0030323064420372248], ['Ġwas', 0.006746355444192886], ['Ġworth', 0.008515341207385063], ['Ġ4', 0.006902794353663921], ['cr', 0.008064743131399155], ['Ġthat', 0.0015530702657997608], ['Ġdidn', 0.004275519400835037], ['âĢ', 0.00469618383795023], ['Ļ', 0.005594531539827585], ['t', 0.004146999213844538], ['Ġtransfer', 0.0023502816911786795], ['Ġin', 0.002540220506489277], [',', 0.005738395731896162], ['Ġand', 0.005186732392758131], ['Ġa', 0.00770903006196022], ['Ġrequired', 0.0039405240677297115], ['Ġfreshman', 0.002967431442812085], ['Ġseminar', 0.003339580027386546], ['Ġfrom', 0.0027315719053149223], ['Ġmy', 0.006350552197545767], ['Ġlast', 0.005859818775206804], ['Ġcollege', 0.0064992839470505714], ['Ġdidn', 0.0063580297864973545], ['âĢ', 0.008633033372461796], ['Ļ', 0.004590916447341442], ['t', 0.006141530815511942], ['Ġtranslate', 0.002403362188488245], ['Ġto', 0.0029228711500763893], ['Ġanything', 0.0037550509441643953], ['.', 0.007021838333457708], ['Ċ', 0.001444980502128601], ['Ċ', 0.005121298599988222], ['It', 0.0031611688900738955], ['âĢ', 0.0029931000899523497], ['Ļ', 0.002266770228743553], ['s', 0.0038968767039477825], ['Ġfair', 0.0026473465841263533], ['.', 0.0031429447699338198], ['ĠBut', 0.004061199724674225], ['Ġit', 0.0027908587362617254], ['Ġsucks', 0.004096176475286484], ['.', 0.2755703330039978], ['</s>', 0.007860892452299595]], [['<s>', 0.26571348309516907], ['The', 0.056379497051239014], ['Ġuniversity', 0.05294588953256607], ['Ġno', 0.02446029521524906], ['Ġlonger', 0.01271286141127348], ['Ġdoes', 0.015582921914756298], ['Ġguaranteed', 0.007383714430034161], ['Ġlongevity', 0.006088178604841232], ['Ġpay', 0.014901518821716309], ['Ġdue', 0.0163430143147707], ['Ġto', 0.023397859185934067], ['Ġannual', 0.01609017327427864], ['Ġincreases', 0.018681924790143967], ['Ġof', 0.009468681178987026], ['Ġthe', 0.022589143365621567], ['Ġminimum', 0.020499497652053833], ['Ġwage', 0.02418750710785389], ['.', 0.06564805656671524], ['Ġ:(', 0.06127270311117172], ['</s>', 0.26565298438072205]]]}\n",
            "\n",
            "Preview of 'pos_tags':\n",
            "{'BostonU_0': [['<s>', 'NN'], ['i', 'NN']], 'BostonU_1': [['<s>', 'VBD'], ['the', 'DT']], 'BostonU_2': [['<s>', 'NN'], ['i', 'NN']]}\n",
            "Processed 0/199874 documents\n",
            "Processed 1000/199874 documents\n",
            "Processed 2000/199874 documents\n",
            "Processed 3000/199874 documents\n",
            "Processed 4000/199874 documents\n",
            "Processed 5000/199874 documents\n",
            "Processed 6000/199874 documents\n",
            "Processed 7000/199874 documents\n",
            "Processed 8000/199874 documents\n",
            "Processed 9000/199874 documents\n",
            "Processed 10000/199874 documents\n",
            "Processed 11000/199874 documents\n",
            "Processed 12000/199874 documents\n",
            "Processed 13000/199874 documents\n",
            "Processed 14000/199874 documents\n",
            "Processed 15000/199874 documents\n",
            "Processed 16000/199874 documents\n",
            "Processed 17000/199874 documents\n",
            "Processed 18000/199874 documents\n",
            "Processed 19000/199874 documents\n",
            "Processed 20000/199874 documents\n",
            "Processed 21000/199874 documents\n",
            "Processed 22000/199874 documents\n",
            "Processed 23000/199874 documents\n",
            "Processed 24000/199874 documents\n",
            "Processed 25000/199874 documents\n",
            "Processed 26000/199874 documents\n",
            "Processed 27000/199874 documents\n",
            "Processed 28000/199874 documents\n",
            "Processed 29000/199874 documents\n",
            "Processed 30000/199874 documents\n",
            "Processed 31000/199874 documents\n",
            "Processed 32000/199874 documents\n",
            "Processed 33000/199874 documents\n",
            "Processed 34000/199874 documents\n",
            "Processed 35000/199874 documents\n",
            "Processed 36000/199874 documents\n",
            "Processed 37000/199874 documents\n",
            "Processed 38000/199874 documents\n",
            "Processed 39000/199874 documents\n",
            "Processed 40000/199874 documents\n",
            "Processed 41000/199874 documents\n",
            "Processed 42000/199874 documents\n",
            "Processed 43000/199874 documents\n",
            "Processed 44000/199874 documents\n",
            "Processed 45000/199874 documents\n",
            "Processed 46000/199874 documents\n",
            "Processed 47000/199874 documents\n",
            "Processed 48000/199874 documents\n",
            "Processed 49000/199874 documents\n",
            "Processed 50000/199874 documents\n",
            "Processed 51000/199874 documents\n",
            "Processed 52000/199874 documents\n",
            "Processed 53000/199874 documents\n",
            "Processed 54000/199874 documents\n",
            "Processed 55000/199874 documents\n",
            "Processed 56000/199874 documents\n",
            "Processed 57000/199874 documents\n",
            "Processed 58000/199874 documents\n",
            "Processed 59000/199874 documents\n",
            "Processed 60000/199874 documents\n",
            "Processed 61000/199874 documents\n",
            "Processed 62000/199874 documents\n",
            "Processed 63000/199874 documents\n",
            "Processed 64000/199874 documents\n",
            "Processed 65000/199874 documents\n",
            "Processed 66000/199874 documents\n",
            "Processed 67000/199874 documents\n",
            "Processed 68000/199874 documents\n",
            "Processed 69000/199874 documents\n",
            "Processed 70000/199874 documents\n",
            "Processed 71000/199874 documents\n",
            "Processed 72000/199874 documents\n",
            "Processed 73000/199874 documents\n",
            "Processed 74000/199874 documents\n",
            "Processed 75000/199874 documents\n",
            "Processed 76000/199874 documents\n",
            "Processed 77000/199874 documents\n",
            "Processed 78000/199874 documents\n",
            "Processed 79000/199874 documents\n",
            "Processed 80000/199874 documents\n",
            "Processed 81000/199874 documents\n",
            "Processed 82000/199874 documents\n",
            "Processed 83000/199874 documents\n",
            "Processed 84000/199874 documents\n",
            "Processed 85000/199874 documents\n",
            "Processed 86000/199874 documents\n",
            "Processed 87000/199874 documents\n",
            "Processed 88000/199874 documents\n",
            "Processed 89000/199874 documents\n",
            "Processed 90000/199874 documents\n",
            "Processed 91000/199874 documents\n",
            "Processed 92000/199874 documents\n",
            "Processed 93000/199874 documents\n",
            "Processed 94000/199874 documents\n",
            "Processed 95000/199874 documents\n",
            "Processed 96000/199874 documents\n",
            "Processed 97000/199874 documents\n",
            "Processed 98000/199874 documents\n",
            "Processed 99000/199874 documents\n",
            "Processed 100000/199874 documents\n",
            "Processed 101000/199874 documents\n",
            "Processed 102000/199874 documents\n",
            "Processed 103000/199874 documents\n",
            "Processed 104000/199874 documents\n",
            "Processed 105000/199874 documents\n",
            "Processed 106000/199874 documents\n",
            "Processed 107000/199874 documents\n",
            "Processed 108000/199874 documents\n",
            "Processed 109000/199874 documents\n",
            "Processed 110000/199874 documents\n",
            "Processed 111000/199874 documents\n",
            "Processed 112000/199874 documents\n",
            "Processed 113000/199874 documents\n",
            "Processed 114000/199874 documents\n",
            "Processed 115000/199874 documents\n",
            "Processed 116000/199874 documents\n",
            "Processed 117000/199874 documents\n",
            "Processed 118000/199874 documents\n",
            "Processed 119000/199874 documents\n",
            "Processed 120000/199874 documents\n",
            "Processed 121000/199874 documents\n",
            "Processed 122000/199874 documents\n",
            "Processed 123000/199874 documents\n",
            "Processed 124000/199874 documents\n",
            "Processed 125000/199874 documents\n",
            "Processed 126000/199874 documents\n",
            "Processed 127000/199874 documents\n",
            "Processed 128000/199874 documents\n",
            "Processed 129000/199874 documents\n",
            "Processed 130000/199874 documents\n",
            "Processed 131000/199874 documents\n",
            "Processed 132000/199874 documents\n",
            "Processed 133000/199874 documents\n",
            "Processed 134000/199874 documents\n",
            "Processed 135000/199874 documents\n",
            "Processed 136000/199874 documents\n",
            "Processed 137000/199874 documents\n",
            "Processed 138000/199874 documents\n",
            "Processed 139000/199874 documents\n",
            "Processed 140000/199874 documents\n",
            "Processed 141000/199874 documents\n",
            "Processed 142000/199874 documents\n",
            "Processed 143000/199874 documents\n",
            "Processed 144000/199874 documents\n",
            "Processed 145000/199874 documents\n",
            "Processed 146000/199874 documents\n",
            "Processed 147000/199874 documents\n",
            "Processed 148000/199874 documents\n",
            "Processed 149000/199874 documents\n",
            "Processed 150000/199874 documents\n",
            "Processed 151000/199874 documents\n",
            "Processed 152000/199874 documents\n",
            "Processed 153000/199874 documents\n",
            "Processed 154000/199874 documents\n",
            "Processed 155000/199874 documents\n",
            "Processed 156000/199874 documents\n",
            "Processed 157000/199874 documents\n",
            "Processed 158000/199874 documents\n",
            "Processed 159000/199874 documents\n",
            "Processed 160000/199874 documents\n",
            "Processed 161000/199874 documents\n",
            "Processed 162000/199874 documents\n",
            "Processed 163000/199874 documents\n",
            "Processed 164000/199874 documents\n",
            "Processed 165000/199874 documents\n",
            "Processed 166000/199874 documents\n",
            "Processed 167000/199874 documents\n",
            "Processed 168000/199874 documents\n",
            "Processed 169000/199874 documents\n",
            "Processed 170000/199874 documents\n",
            "Processed 171000/199874 documents\n",
            "Processed 172000/199874 documents\n",
            "Processed 173000/199874 documents\n",
            "Processed 174000/199874 documents\n",
            "Processed 175000/199874 documents\n",
            "Processed 176000/199874 documents\n",
            "Processed 177000/199874 documents\n",
            "Processed 178000/199874 documents\n",
            "Processed 179000/199874 documents\n",
            "Processed 180000/199874 documents\n",
            "Processed 181000/199874 documents\n",
            "Processed 182000/199874 documents\n",
            "Processed 183000/199874 documents\n",
            "Processed 184000/199874 documents\n",
            "Processed 185000/199874 documents\n",
            "Processed 186000/199874 documents\n",
            "Processed 187000/199874 documents\n",
            "Processed 188000/199874 documents\n",
            "Processed 189000/199874 documents\n",
            "Processed 190000/199874 documents\n",
            "Processed 191000/199874 documents\n",
            "Processed 192000/199874 documents\n",
            "Processed 193000/199874 documents\n",
            "Processed 194000/199874 documents\n",
            "Processed 195000/199874 documents\n",
            "Processed 196000/199874 documents\n",
            "Processed 197000/199874 documents\n",
            "Processed 198000/199874 documents\n",
            "Processed 199000/199874 documents\n",
            "\n",
            "Examples of top-ranked nouns and verbs before clustering:\n",
            "Document 0:\n",
            "ġwere (0.01), ġclasses (0.01)\n",
            "Document 1:\n",
            "ġuniversity (0.05)\n",
            "Document 2:\n",
            "ġhe (0.03)\n",
            "Document 3:\n",
            "ġi (0.04)\n",
            "Document 4:\n",
            "ġhal (0.03)\n",
            "\n",
            "Created matrix of shape (199133, 14547)\n",
            "\n",
            "Processing matrix of shape (199133, 14547) with 4 clusters\n",
            "CUDA not available - using CPU\n",
            "\n",
            "Analyzing clusters...\n",
            "\n",
            "Cluster 0:\n",
            "Size: 9365 posts (4.7%)\n",
            "Top words (weight): ġ(685.63), ċ(36.49), i(3.62), ġi(3.56), gt(1.82)\n",
            "Sample post: Uh oh sounds like Wendy Wintersteen is about to send her goons with tire irons \n",
            "\n",
            "jk just ignore it and they'll probably take you off their list...\n",
            "\n",
            "Cluster 1:\n",
            "Size: 3 posts (0.0%)\n",
            "Top words (weight): ġgoat(0.29)\n",
            "Sample post: That's a goat...\n",
            "\n",
            "Cluster 2:\n",
            "Size: 189477 posts (95.2%)\n",
            "Top words (weight): i(801.81), ġi(554.64), ċ(287.34), ġyou(184.29), ġ(161.24)\n",
            "Sample post: You're absolutely right. \n",
            "\n",
            "We'll agree to disagree and you bet your ass I'll write you off as another stupid Berkeley kid with his or her head up their own ass. You've contradicted yourself from the v...\n",
            "\n",
            "Cluster 3:\n",
            "Size: 288 posts (0.1%)\n",
            "Top words (weight): don(22.65), ċ(0.04), thanks(0.02), ġ(0.02)\n",
            "Sample post: Don’t do it... not even required for the cpa exam...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def visualize_clusters(cluster_results_path: Path = config.output_path / 'theme_clusters.json'):\n",
        "    \"\"\"Visualize the clusters using word clouds\"\"\"\n",
        "    with open(cluster_results_path) as f:\n",
        "        clusters = json.load(f)\n",
        "\n",
        "    for cluster_id, data in clusters.items():\n",
        "        # Create word frequency dict from tokens\n",
        "        text = ' '.join(data['tokens'])\n",
        "\n",
        "        # Generate word cloud\n",
        "        wordcloud = WordCloud(\n",
        "            width=800,\n",
        "            height=400,\n",
        "            background_color='white',\n",
        "            colormap='viridis'\n",
        "        ).generate(text)\n",
        "\n",
        "        # Display\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Theme Cluster {cluster_id}')\n",
        "        plt.show()\n",
        "\n",
        "        # Print some statistics\n",
        "        print(f\"\\nCluster {cluster_id} Statistics:\")\n",
        "        colleges = data['colleges']\n",
        "        college_counts = pd.Series(colleges).value_counts()\n",
        "        print(\"\\nTop colleges in this cluster:\")\n",
        "        print(college_counts.head())\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "visualize_clusters()"
      ],
      "metadata": {
        "id": "Cj5nvWbSXTLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}